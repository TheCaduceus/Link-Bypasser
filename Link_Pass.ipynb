{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheCaduceus/Link-Bypasser/blob/main/Link_Pass.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter AdFly Link Below!**"
      ],
      "metadata": {
        "id": "rWggyWOjc2vK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown <center><img src='https://drfilestreambot.cf/AgADRA14795' height=\"100\">\n",
        "#@title <b><center>Enter AdFly Link Below</center></b>\n",
        "!pip install cloudscraper\n",
        "import cloudscraper\n",
        "import re\n",
        "from base64 import b64decode\n",
        "from urllib.parse import unquote\n",
        "\n",
        "# ==========================================\n",
        "\n",
        "url = \"\" #@param {type:\"string\"}\n",
        "'''\n",
        "404: Complete exception handling not found :(\n",
        "'''\n",
        "# ==========================================\n",
        "\n",
        "def decrypt_url(code):\n",
        "    a, b = '', ''\n",
        "    for i in range(0, len(code)):\n",
        "        if i % 2 == 0: a += code[i]\n",
        "        else: b = code[i] + b\n",
        "    key = list(a + b)\n",
        "    i = 0\n",
        "    while i < len(key):\n",
        "        if key[i].isdigit():\n",
        "            for j in range(i+1,len(key)):\n",
        "                if key[j].isdigit():\n",
        "                    u = int(key[i]) ^ int(key[j])\n",
        "                    if u < 10: key[i] = str(u)\n",
        "                    i = j\t\t\t\t\t\n",
        "                    break\n",
        "        i+=1\n",
        "    key = ''.join(key)\n",
        "    decrypted = b64decode(key)[16:-16]\n",
        "    return decrypted.decode('utf-8')\n",
        "\n",
        "# ==========================================\n",
        "\n",
        "def adfly(url):\n",
        "    client = cloudscraper.create_scraper(allow_brotli=False)\n",
        "    res = client.get(url).text\n",
        "    out = {'error': False, 'src_url': url}\n",
        "    try:\n",
        "        ysmm = re.findall(\"ysmm\\s+=\\s+['|\\\"](.*?)['|\\\"]\", res)[0]\n",
        "    except:\n",
        "        out['error'] = True\n",
        "        return out\n",
        "    url = decrypt_url(ysmm)\n",
        "    if re.search(r'go\\.php\\?u\\=', url):\n",
        "        url = b64decode(re.sub(r'(.*?)u=', '', url)).decode()\n",
        "    elif '&dest=' in url:\n",
        "        url = unquote(re.sub(r'(.*?)dest=', '', url))\n",
        "    out['bypassed_url'] = url\n",
        "    return out\n",
        "\n",
        "# ==========================================\n",
        "\n",
        "res = adfly(url)\n",
        "\n",
        "print(res)"
      ],
      "metadata": {
        "id": "NRE5wW9wc85c",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter GPLinks Link Below!**"
      ],
      "metadata": {
        "id": "vIuY_F5gVVm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown <center><img src='https://drfilestreambot.cf/AgADRQ14796' height=\"100\">\n",
        "#@title <b><center>Enter GPLinks Link Below</center></b>\n",
        "print(\"Downloading Cloud-Scraper...\")\n",
        "!pip install cloudscraper\n",
        "print(\"Setting Up!\")\n",
        "print(\"Performing Check...\")\n",
        "import time\n",
        "import cloudscraper\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "print(\"Everything Looks Good! Lets Continue.\")\n",
        "\n",
        "url = \"\" #@param {type:\"string\"} \n",
        "print(\"Entered Link:\")\n",
        "print(url)\n",
        "print(\"Checking Link...\")\n",
        "print(\"Checking Done!\")\n",
        "print(\"Bypassing Link...\")\n",
        "# ==============================================\n",
        "\n",
        "def gplinks(url: str):\n",
        "    client = cloudscraper.create_scraper(allow_brotli=False)\n",
        "    p = urlparse(url)\n",
        "    final_url = f\"{p.scheme}://{p.netloc}/links/go\"\n",
        "    res = client.head(url)\n",
        "    header_loc = res.headers[\"location\"]\n",
        "    url = url[:-1] if url[-1] == '/' else url\n",
        "    param = url.split("/")[-1]\n",
        "    req_url = f\"{p.scheme}://{p.netloc}/{param}\"\n",
        "    p = urlparse(header_loc)\n",
        "    ref_url = f\"{p.scheme}://{p.netloc}/\"\n",
        "    h = {\"referer\": ref_url}\n",
        "    res = client.get(req_url, headers=h, allow_redirects=False)\n",
        "    bs4 = BeautifulSoup(res.content, \"html.parser\")\n",
        "    inputs = bs4.find_all(\"input\")\n",
        "    time.sleep(10) # !important\n",
        "    data = { input.get(\"name\"): input.get(\"value\") for input in inputs }\n",
        "    h = {\n",
        "        \"content-type\": \"application/x-www-form-urlencoded\",\n",
        "        \"x-requested-with\": \"XMLHttpRequest\"\n",
        "    }\n",
        "    time.sleep(10)\n",
        "    res = client.post(final_url, headers=h, data=data)\n",
        "    try:\n",
        "        return res.json()[\"url\"].replace(\"/\",\"/\")\n",
        "    except: \n",
        "        return \"Could not Bypass your URL :(\"\n",
        "\n",
        "# ==============================================\n",
        "\n",
        "res = gplinks(url)\n",
        "\n",
        "print(res)\n",
        "print(\"Successfully Bypassed!\")"
      ],
      "metadata": {
        "id": "PHCG2iGQgDhz",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter GDTot Link as well as your GDTot Crypt! If you don't know how to get Crypt then <a href=\"https://www.youtube.com/watch?v=EfZ29CotRSU\">Learn Here</a>**"
      ],
      "metadata": {
        "id": "cyBiaGkAUtLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown <center><img src='https://drfilestreambot.cf/AgADQg14793' height=\"100\">\n",
        "#@title <b><center>Enter GDTot-Link Below</center></b>\n",
        "!pip install cloudscraper\n",
        "import cloudscraper\n",
        "import re\n",
        "import base64\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "\n",
        "url = \"\" #@param {type:\"string\"}\n",
        "GDTot_Crypt = \"b0lDek5LSCt6ZjVRR2EwZnY4T1EvVndqeDRtbCtTWmMwcGNuKy8wYWpDaz0%3D\" #@param {type:\"string\"}\n",
        "# ==========================================\n",
        "\n",
        "def gdtot(url):\n",
        "    client = cloudscraper.create_scraper(allow_brotli=False)\n",
        "    match = re.findall(r\"https?://(.+)\\.gdtot\\.(.+)\\/\\S+\\/\\S+\", url)[0]\n",
        "    client.cookies.update({ \"crypt\": GDTot_Crypt })\n",
        "    res = client.get(url)\n",
        "    res = client.get(f\"https://{match[0]}.gdtot.{match[1]}/dld?id={url.split('/')[-1]}\")\n",
        "    url = re.findall(r'URL=(.*?)\"', res.text)[0]\n",
        "    info = {}\n",
        "    info[\"error\"] = False\n",
        "    params = parse_qs(urlparse(url).query)\n",
        "    if \"gd\" not in params or not params[\"gd\"] or params[\"gd\"][0] == \"false\":\n",
        "        info[\"error\"] = True\n",
        "        if \"msgx\" in params:\n",
        "            info[\"message\"] = params[\"msgx\"][0]\n",
        "        else:\n",
        "            info[\"message\"] = \"Invalid link\"\n",
        "    else:\n",
        "        decoded_id = base64.b64decode(str(params[\"gd\"][0])).decode(\"utf-8\")\n",
        "        drive_link = f\"https://drive.google.com/open?id={decoded_id}\"\n",
        "        info[\"gdrive_link\"] = drive_link\n",
        "    if not info[\"error\"]:\n",
        "        return info[\"gdrive_link\"]\n",
        "    else:\n",
        "        return \"Could not generate GDrive URL for your GDTot Link :(\"\n",
        "\n",
        "# ==========================================\n",
        "\n",
        "res = gdtot(url)\n",
        "\n",
        "print(res)"
      ],
      "metadata": {
        "id": "pJCdd8LESBAk",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter Sharer.pw Link, XSRF_TOKEN and laravel_session cookies! If you don't know how to get then then watch this <a href=\"https://www.youtube.com/watch?v=EfZ29CotRSU\">Video</a> (for GDTOT) and do the same for Sharer.pw**"
      ],
      "metadata": {
        "id": "JlOUDYIzlLTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown <center><img src='https://drfilestreambot.cf/AgADRg14797' height=\"50\">\n",
        "#@title <b><center>Enter Sharer.pw Link Below</center></b>\n",
        "!pip install cloudscraper\n",
        "import cloudscraper\n",
        "import re\n",
        "from lxml import etree\n",
        "\n",
        "url = \"\" #@param {type:\"string\"}\n",
        "XSRF_TOKEN = \"\" #@param {type:\"string\"}\n",
        "Laravel_Session = \"\" #@param {type:\"string\"}\n",
        "'''\n",
        "404: Exception Handling Not Found :(\n",
        "NOTE:\n",
        "DO NOT use the logout button on website. Instead, clear the site cookies manually to log out.\n",
        "If you use logout from website, cookies will become invalid.\n",
        "'''\n",
        "\n",
        "# ===================================================================\n",
        "\n",
        "def parse_info(res):\n",
        "    f = re.findall(\">(.*?)<\\/td>\", res.text)\n",
        "    info_parsed = {}\n",
        "    for i in range(0, len(f), 3):\n",
        "        info_parsed[f[i].lower().replace(' ', '_')] = f[i+2]\n",
        "    return info_parsed\n",
        "\n",
        "def sharer_pw(url, forced_login=False):\n",
        "    client = cloudscraper.create_scraper(allow_brotli=False)\n",
        "    client.cookies.update({\n",
        "        \"XSRF-TOKEN\": XSRF_TOKEN,\n",
        "        \"laravel_session\": Laravel_Session\n",
        "    })\n",
        "    res = client.get(url)\n",
        "    token = re.findall(\"_token\\s=\\s'(.*?)'\", res.text, re.DOTALL)[0]\n",
        "    ddl_btn = etree.HTML(res.content).xpath(\"//button[@id='btndirect']\")\n",
        "    info_parsed = parse_info(res)\n",
        "    info_parsed['error'] = True\n",
        "    info_parsed['src_url'] = url\n",
        "    info_parsed['link_type'] = 'login'\n",
        "    info_parsed['forced_login'] = forced_login\n",
        "    headers = {\n",
        "        'content-type': 'application/x-www-form-urlencoded; charset=UTF-8',\n",
        "        'x-requested-with': 'XMLHttpRequest'\n",
        "    }\n",
        "    data = {\n",
        "        '_token': token\n",
        "    }\n",
        "    if len(ddl_btn):\n",
        "        info_parsed['link_type'] = 'direct'\n",
        "    if not forced_login:\n",
        "        data['nl'] = 1\n",
        "    try: \n",
        "        res = client.post(url+'/dl', headers=headers, data=data).json()\n",
        "    except:\n",
        "        return info_parsed\n",
        "    if 'url' in res and res['url']:\n",
        "        info_parsed['error'] = False\n",
        "        info_parsed['gdrive_link'] = res['url']\n",
        "    if len(ddl_btn) and not forced_login and not 'url' in info_parsed:\n",
        "        # retry download via login\n",
        "        return sharer_pw(url, forced_login=True)\n",
        "    return info_parsed\n",
        "\n",
        "# ===================================================================\n",
        "\n",
        "res = sharer_pw(url)\n",
        "\n",
        "print(res)"
      ],
      "metadata": {
        "id": "jY_RrpdYiTqj",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter DropLink Below!**"
      ],
      "metadata": {
        "id": "2PlnCgEllyT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown <center><img src='https://drfilestreambot.cf/AgADRw14798' height=\"75\">\n",
        "#@title <b><center>Enter Drop Link Below</center></b>\n",
        "!pip install cloudscraper\n",
        "import cloudscraper\n",
        "\n",
        "url = \"\" #@param {type:\"string\"}\n",
        "# ==============================================\n",
        "\n",
        "def droplink(url):\n",
        "    api = \"https://api.emilyx.in/api\"\n",
        "    client = cloudscraper.create_scraper(allow_brotli=False)\n",
        "    resp = client.get(url)\n",
        "    if resp.status_code == 404:\n",
        "        return \"File not found/The link you entered is wrong!\"\n",
        "    try:\n",
        "        resp = client.post(api, json={\"type\": \"droplink\", \"url\": url})\n",
        "        res = resp.json()\n",
        "    except BaseException:\n",
        "        return \"API UnResponsive / Invalid Link !\"\n",
        "    if res[\"success\"] is True:\n",
        "        return res[\"url\"]\n",
        "    else:\n",
        "        return res[\"msg\"]\n",
        "\n",
        "# ==============================================\n",
        "\n",
        "res = droplink(url)\n",
        "\n",
        "print(res)"
      ],
      "metadata": {
        "id": "611_HcrXfOOr",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter AppDrive or DriveApp etc. Look-Alike Link and as well as the Account Details (Required for Login Required Links only)**"
      ],
      "metadata": {
        "id": "WlRAIhcUoVHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown <center><img src='https://drfilestreambot.cf/AgADSA14799' height=\"85\">\n",
        "#@title <b><center>Enter App Drive or Drive App Link Below</center></b>\n",
        "print(\"Downloading Cloud-Scraper...\")\n",
        "!pip install cloudscraper\n",
        "import cloudscraper\n",
        "import re\n",
        "import requests\n",
        "from lxml import etree\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "\n",
        "# Website User Account (NOT GOOGLE ACCOUNT) ----\n",
        "url = \"\" #@param {type:\"string\"}\n",
        "Email = \"OPTIONAL\" #@param {type:\"string\"}\n",
        "Password = \"OPTIONAL\" #@param {type:\"string\"}\n",
        "\n",
        "'''\n",
        "NOTE: \n",
        " - Auto-detection for non-login urls, and indicated via 'link_type' (direct/login) in output.\n",
        "SUPPORTED DOMAINS:\n",
        " - appdrive.in\n",
        " - driveapp.in\n",
        " - drivehub.in\n",
        " - gdflix.pro\n",
        " - drivesharer.in\n",
        " - drivebit.in\n",
        " - drivelinks.in\n",
        " - driveace.in\n",
        " - drivepro.in\n",
        " \n",
        "'''\n",
        "print(\"Generating GDrive Link...\")\n",
        "\n",
        "# ===================================================================\n",
        "\n",
        "def unified(url):\n",
        "    try:\n",
        "        account = {\"email\": Email, \"passwd\": Password}\n",
        "        client = cloudscraper.create_scraper(allow_brotli=False)\n",
        "        client.headers.update(\n",
        "            {\n",
        "                \"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36\"\n",
        "            }\n",
        "        )\n",
        "        data = {\"email\": account[\"email\"], \"password\": account[\"passwd\"]}\n",
        "        client.post(f\"https://{urlparse(url).netloc}/login\", data=data)\n",
        "        res = client.get(url)\n",
        "        key = re.findall('\"key\",\\s+\"(.*?)\"', res.text)[0]\n",
        "        ddl_btn = etree.HTML(res.content).xpath(\"//button[@id='drc']\")\n",
        "        info = re.findall(\">(.*?)<\\/li>\", res.text)\n",
        "        info_parsed = {}\n",
        "        for item in info:\n",
        "            kv = [s.strip() for s in item.split(\":\", maxsplit=1)]\n",
        "            info_parsed[kv[0].lower()] = kv[1]\n",
        "        info_parsed = info_parsed\n",
        "        info_parsed[\"error\"] = False\n",
        "        info_parsed[\"link_type\"] = \"login\"\n",
        "        headers = {\n",
        "            \"Content-Type\": f\"multipart/form-data; boundary={'-'*4}_\",\n",
        "        }\n",
        "        data = {\"type\": 1, \"key\": key, \"action\": \"original\"}\n",
        "        if len(ddl_btn):\n",
        "            info_parsed[\"link_type\"] = \"direct\"\n",
        "            data[\"action\"] = \"direct\"\n",
        "        while data[\"type\"] <= 3:\n",
        "            boundary = f'{\"-\"*6}_'\n",
        "            data_string = \"\"\n",
        "            for item in data:\n",
        "                data_string += f\"{boundary}\\r\\n\"\n",
        "                data_string += f'Content-Disposition: form-data; name=\"{item}\"\\r\\n\\r\\n{data[item]}\\r\\n'\n",
        "            data_string += f\"{boundary}--\\r\\n\"\n",
        "            gen_payload = data_string\n",
        "            try:\n",
        "                response = client.post(url, data=gen_payload, headers=headers).json()\n",
        "                break\n",
        "            except BaseException:\n",
        "                data[\"type\"] += 1\n",
        "        if \"url\" in response:\n",
        "            info_parsed[\"gdrive_link\"] = response[\"url\"]\n",
        "        elif \"error\" in response and response[\"error\"]:\n",
        "            info_parsed[\"error\"] = True\n",
        "            info_parsed[\"error_message\"] = response[\"message\"]\n",
        "        else:\n",
        "            info_parsed[\"error\"] = True\n",
        "            info_parsed[\"error_message\"] = \"Something went wrong :(\"\n",
        "        if info_parsed[\"error\"]:\n",
        "            return info_parsed\n",
        "        if urlparse(url).netloc == \"driveapp.in\" and not info_parsed[\"error\"]:\n",
        "            res = client.get(info_parsed[\"gdrive_link\"])\n",
        "            drive_link = etree.HTML(res.content).xpath(\n",
        "                \"//a[contains(@class,'btn')]/@href\"\n",
        "            )[0]\n",
        "            info_parsed[\"gdrive_link\"] = drive_link\n",
        "        info_parsed[\"src_url\"] = url\n",
        "        if urlparse(url).netloc == \"drivehub.in\" and not info_parsed[\"error\"]:\n",
        "            res = client.get(info_parsed[\"gdrive_link\"])\n",
        "            drive_link = etree.HTML(res.content).xpath(\n",
        "                \"//a[contains(@class,'btn')]/@href\"\n",
        "            )[0]\n",
        "            info_parsed[\"gdrive_link\"] = drive_link\n",
        "        if urlparse(url).netloc == \"gdflix.pro\" and not info_parsed[\"error\"]:\n",
        "            res = client.get(info_parsed[\"gdrive_link\"])\n",
        "            drive_link = etree.HTML(res.content).xpath(\n",
        "                \"//a[contains(@class,'btn')]/@href\"\n",
        "            )[0]\n",
        "            info_parsed[\"gdrive_link\"] = drive_link\n",
        "\n",
        "        if urlparse(url).netloc == \"drivesharer.in\" and not info_parsed[\"error\"]:\n",
        "            res = client.get(info_parsed[\"gdrive_link\"])\n",
        "            drive_link = etree.HTML(res.content).xpath(\n",
        "                \"//a[contains(@class,'btn')]/@href\"\n",
        "            )[0]\n",
        "            info_parsed[\"gdrive_link\"] = drive_link\n",
        "        if urlparse(url).netloc == \"drivebit.in\" and not info_parsed[\"error\"]:\n",
        "            res = client.get(info_parsed[\"gdrive_link\"])\n",
        "            drive_link = etree.HTML(res.content).xpath(\n",
        "                \"//a[contains(@class,'btn')]/@href\"\n",
        "            )[0]\n",
        "            info_parsed[\"gdrive_link\"] = drive_link\n",
        "        if urlparse(url).netloc == \"drivelinks.in\" and not info_parsed[\"error\"]:\n",
        "            res = client.get(info_parsed[\"gdrive_link\"])\n",
        "            drive_link = etree.HTML(res.content).xpath(\n",
        "                \"//a[contains(@class,'btn')]/@href\"\n",
        "            )[0]\n",
        "            info_parsed[\"gdrive_link\"] = drive_link\n",
        "        if urlparse(url).netloc == \"driveace.in\" and not info_parsed[\"error\"]:\n",
        "            res = client.get(info_parsed[\"gdrive_link\"])\n",
        "            drive_link = etree.HTML(res.content).xpath(\n",
        "                \"//a[contains(@class,'btn')]/@href\"\n",
        "            )[0]\n",
        "            info_parsed[\"gdrive_link\"] = drive_link\n",
        "        if urlparse(url).netloc == \"drivepro.in\" and not info_parsed[\"error\"]:\n",
        "            res = client.get(info_parsed[\"gdrive_link\"])\n",
        "            drive_link = etree.HTML(res.content).xpath(\n",
        "                \"//a[contains(@class,'btn')]/@href\"\n",
        "            )[0]\n",
        "            info_parsed[\"gdrive_link\"] = drive_link\n",
        "        if info_parsed[\"error\"]:\n",
        "            return \"Faced an Unknown Error!\"\n",
        "        return info_parsed[\"gdrive_link\"]\n",
        "    except BaseException:\n",
        "        return \"Unable to Extract GDrive Link\"\n",
        "\n",
        "# ===================================================================\n",
        "\n",
        "res = unified(url)\n",
        "\n",
        "print(res)"
      ],
      "metadata": {
        "id": "0vqE8a8dm5T4",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter Linkvertise Link Below!**"
      ],
      "metadata": {
        "id": "OY1CtzT0pA8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown <center><img src='https://drfilestreambot.cf/AgADSQ14800' height=\"50\">\n",
        "#@title <b><center>Enter Linkvertise-Link Below</center></b>\n",
        "print(\"Downloading Cloud-Scraper...\")\n",
        "!pip install cloudscraper\n",
        "print(\"Importing Files!\")\n",
        "import cloudscraper\n",
        "\n",
        "url = \"\" #@param {type:\"string\"}\n",
        "print(\"You have Entered:\")\n",
        "print(\"URL:\")\n",
        "print(url)\n",
        "print(\"Bypassing the Link...\")\n",
        "# -------------------------------------------\n",
        "\n",
        "def linkvertise(url):\n",
        "    api = \"https://api.emilyx.in/api\"\n",
        "    client = cloudscraper.create_scraper(allow_brotli=False)\n",
        "    resp = client.get(url)\n",
        "    if resp.status_code == 404:\n",
        "        return \"File not found/The link you entered is wrong!\"\n",
        "    try:\n",
        "        resp = client.post(api, json={\"type\": \"linkvertise\", \"url\": url})\n",
        "        res = resp.json()\n",
        "    except BaseException:\n",
        "        return \"API UnResponsive / Invalid Link !\"\n",
        "    if res[\"success\"] is True:\n",
        "        return res[\"url\"]\n",
        "    else:\n",
        "        return res[\"msg\"]\n",
        "\n",
        "# -------------------------------------------\n",
        "\n",
        "res = linkvertise(url)\n",
        "\n",
        "print(res)\n",
        "print(\"Successfully Bypassed!\")"
      ],
      "metadata": {
        "id": "4yV_DpjXpBXj",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter RockLinks Link Below!**"
      ],
      "metadata": {
        "id": "IVaFnZycUqSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown <center><img src='https://drfilestreambot.cf/AgADSw14802' height=\"70\">\n",
        "#@title <b><center>Enter Rocklinks-Link Below</center></b>\n",
        "print(\"Downloading Cloud-Scraper...\")\n",
        "!pip install cloudscraper\n",
        "print(\"Setting Up!\")\n",
        "print(\"Performing Check...\")\n",
        "import time\n",
        "import cloudscraper\n",
        "from bs4 import BeautifulSoup \n",
        "print(\"Everything Looks Good! Lets Continue.\")\n",
        "\n",
        "url = \"\"  #@param {type:\"string\"}\n",
        "\n",
        " \n",
        "\n",
        "# ---------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def bypass(url):\n",
        "    client = cloudscraper.create_scraper(allow_brotli=False)\n",
        "    if 'rocklinks.net' in url:\n",
        "        DOMAIN = \"https://blog.disheye.com\"\n",
        "    else:\n",
        "        DOMAIN = \"https://rocklinks.net\"\n",
        "\n",
        "    url = url[:-1] if url[-1] == '/' else url\n",
        "\n",
        "    code = url.split(\"/\")[-1]\n",
        "    if 'rocklinks.net' in url:\n",
        "        final_url = f\"{DOMAIN}/{code}?quelle=\" \n",
        "    else:\n",
        "        final_url = f\"{DOMAIN}/{code}\"\n",
        "\n",
        "    resp = client.get(final_url)\n",
        "    soup = BeautifulSoup(resp.content, \"html.parser\")\n",
        "    \n",
        "    try: inputs = soup.find(id=\"go-link\").find_all(name=\"input\")\n",
        "    except: return \"Incorrect Link\"\n",
        "    \n",
        "    data = { input.get('name'): input.get('value') for input in inputs }\n",
        "\n",
        "    h = { \"x-requested-with\": \"XMLHttpRequest\" }\n",
        "    \n",
        "    time.sleep(10)\n",
        "    r = client.post(f\"{DOMAIN}/links/go\", data=data, headers=h)\n",
        "    try:\n",
        "        return r.json()['url']\n",
        "    except: return \"Something went wrong :(\"\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------------------------------\n",
        "res = bypass(url)\n",
        "\n",
        "print(res)\n",
        "print(\"Successfully Bypassed!\")"
      ],
      "metadata": {
        "id": "bG5bvLAxUui3",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter Gyanilinks Link Below!**"
      ],
      "metadata": {
        "id": "hzP92k6eACgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown <center><img src='https://drfilestreambot.cf/AgADTw14808' height=\"50\">\n",
        "#@title <b><center>Enter gyanilinks Link Below</center></b>\n",
        "print(\"Downloading Cloud-Scraper...\")\n",
        "!pip install cloudscraper\n",
        "print(\"Setting Up!\")\n",
        "print(\"Performing Check...\")\n",
        "import time\n",
        "import cloudscraper\n",
        "from bs4 import BeautifulSoup \n",
        "print(\"Everything Looks Good! Lets Continue.\")\n",
        "\n",
        "url = \"\"  #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "'''\n",
        "NOTE: \n",
        "SUPPORTED DOMAINS:\n",
        " - gtlinks.me\n",
        " \n",
        "'''\n",
        "# ---------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def bypass(url):\n",
        "    client = cloudscraper.create_scraper(allow_brotli=False)\n",
        "    if 'gtlinks.me' in url:\n",
        "        DOMAIN = \"https://go.gyanitheme.com\"\n",
        "    else:\n",
        "        return \"Incorrect Link\"\n",
        "\n",
        "    url = url[:-1] if url[-1] == '/' else url\n",
        "\n",
        "    code = url.split(\"/\")[-1]\n",
        "    \n",
        "    final_url = f\"{DOMAIN}/{code}\"\n",
        "\n",
        "    resp = client.get(final_url)\n",
        "    soup = BeautifulSoup(resp.content, \"html.parser\")\n",
        "    \n",
        "    try: inputs = soup.find(id=\"go-link\").find_all(name=\"input\")\n",
        "    except: return \"Incorrect Link\"\n",
        "    \n",
        "    data = { input.get('name'): input.get('value') for input in inputs }\n",
        "\n",
        "    h = { \"x-requested-with\": \"XMLHttpRequest\" }\n",
        "    \n",
        "    time.sleep(5)\n",
        "    r = client.post(f\"{DOMAIN}/links/go\", data=data, headers=h)\n",
        "    try:\n",
        "        return r.json()['url']\n",
        "    except: return \"Something went wrong :(\"\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "res = bypass(url)\n",
        "\n",
        "print(res)\n",
        "print(\"Successfully Bypassed!\")"
      ],
      "metadata": {
        "id": "_iWf_OF9jOKq",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter Shortingly Link Below!**"
      ],
      "metadata": {
        "id": "ew-UlUfi_eSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown <center><img src='https://drfilestreambot.cf/AgADUA14809' height=\"50\">\n",
        "#@title <b><center>Enter shortingly Link Below</center></b>\n",
        "print(\"Downloading Cloud-Scraper...\")\n",
        "!pip install cloudscraper\n",
        "print(\"Setting Up!\")\n",
        "print(\"Performing Check...\")\n",
        "import time\n",
        "import cloudscraper\n",
        "from bs4 import BeautifulSoup \n",
        "print(\"Everything Looks Good! Lets Continue.\")\n",
        "\n",
        "url = \"\"  #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "'''\n",
        "NOTE: \n",
        "SUPPORTED DOMAINS:\n",
        " - shortingly.me\n",
        " \n",
        "'''\n",
        "# ---------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def bypass(url):\n",
        "    client = cloudscraper.create_scraper(allow_brotli=False)\n",
        "    if 'shortingly.me' in url:\n",
        "        DOMAIN = \"https://go.techyjeeshan.xyz\"\n",
        "    else:\n",
        "        return \"Incorrect Link\"\n",
        "\n",
        "    url = url[:-1] if url[-1] == '/' else url\n",
        "\n",
        "    code = url.split(\"/\")[-1]\n",
        "    \n",
        "    final_url = f\"{DOMAIN}/{code}\"\n",
        "\n",
        "    resp = client.get(final_url)\n",
        "    soup = BeautifulSoup(resp.content, \"html.parser\")\n",
        "    \n",
        "    try: inputs = soup.find(id=\"go-link\").find_all(name=\"input\")\n",
        "    except: return \"Incorrect Link\"\n",
        "    \n",
        "    data = { input.get('name'): input.get('value') for input in inputs }\n",
        "\n",
        "    h = { \"x-requested-with\": \"XMLHttpRequest\" }\n",
        "    \n",
        "    time.sleep(5)\n",
        "    r = client.post(f\"{DOMAIN}/links/go\", data=data, headers=h)\n",
        "    try:\n",
        "        return r.json()['url']\n",
        "    except: return \"Something went wrong :(\"\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "res = bypass(url)\n",
        "\n",
        "print(res)\n",
        "print(\"Successfully Bypassed!\")"
      ],
      "metadata": {
        "id": "VLQNAB35oAGZ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter ShareUs Link Below!**"
      ],
      "metadata": {
        "id": "xOd-WNFWhs6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b><center>Enter ShareUs Link Below</center></b>\n",
        "#@markdown <center><img src='https://drfilestreambot.cf/AgADUQ14810' height=\"50\">\n",
        "import requests\n",
        "\n",
        "url = \"https://shareus.in/?i=y3wWSo\" #@param {type:\"string\"}\n",
        "token = url.split(\"=\")[-1]\n",
        "\n",
        "bypassed_url = \"https://us-central1-my-apps-server.cloudfunctions.net/r?shortid=\"+ token\n",
        "response = requests.get(bypassed_url).text\n",
        "print(response)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "nMbSeTR8gVdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter PSA link Below!**"
      ],
      "metadata": {
        "id": "eUj7FRLCxT9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b><center>Enter PSA Link Below</center></b>\n",
        "#@markdown <center><img src='https://drfilestreambot.cf/AgADVA14814' height=\"70\">\n",
        "\n",
        "!pip install cloudscraper\n",
        "!pip install bs4\n",
        "\n",
        "import cloudscraper\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import re\n",
        "\n",
        "def try2link_bypass(url):\n",
        "\tclient = cloudscraper.create_scraper(allow_brotli=False)\n",
        "\t\n",
        "\turl = url[:-1] if url[-1] == '/' else url\n",
        "\t\n",
        "\tparams = (('d', int(time.time()) + (60 * 4)),)\n",
        "\tr = client.get(url, params=params, headers= {'Referer': 'https://newforex.online/'})\n",
        "\t\n",
        "\tsoup = BeautifulSoup(r.text, 'html.parser')\n",
        "\tinputs = soup.find(id=\"go-link\").find_all(name=\"input\")\n",
        "\tdata = { input.get('name'): input.get('value') for input in inputs }\t\n",
        "\ttime.sleep(7)\n",
        "\t\n",
        "\theaders = {'Host': 'try2link.com', 'X-Requested-With': 'XMLHttpRequest', 'Origin': 'https://try2link.com', 'Referer': url}\n",
        "\t\n",
        "\tbypassed_url = client.post('https://try2link.com/links/go', headers=headers,data=data)\n",
        "\treturn bypassed_url.json()[\"url\"]\n",
        "\t\t\n",
        "\n",
        "def try2link_scrape(url):\n",
        "\tclient = cloudscraper.create_scraper(allow_brotli=False)\t\n",
        "\th = {\n",
        "\t'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36',\n",
        "\t}\n",
        "\tres = client.get(url, cookies={}, headers=h)\n",
        "\turl = 'https://try2link.com/'+re.findall('try2link\\.com\\/(.*?) ', res.text)[0]\n",
        "\treturn try2link_bypass(url)\n",
        "    \n",
        "\n",
        "def psa_bypasser(psa_url):\n",
        "    client = cloudscraper.create_scraper(allow_brotli=False)\n",
        "    r = client.get(psa_url)\n",
        "    soup = BeautifulSoup(r.text, \"html.parser\").find_all(class_=\"dropshadowboxes-drop-shadow dropshadowboxes-rounded-corners dropshadowboxes-inside-and-outside-shadow dropshadowboxes-lifted-both dropshadowboxes-effect-default\")\n",
        "    links = \"\"\n",
        "    for link in soup:\n",
        "        try:\n",
        "            exit_gate = link.a.get(\"href\")\n",
        "            links = links + try2link_scrape(exit_gate) + '\\n'\n",
        "        except: pass\n",
        "    return links\n",
        "\n",
        "url = \"https://psa.pm/movie/the-infernal-machine-2022/\" #@param {type:\"string\"}\n",
        "print(psa_bypasser(url))\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Mo7njiR0xCXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter OUO Link Below!**"
      ],
      "metadata": {
        "id": "0FUIhDVl0Xri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b><center>Enter OUO Link Below</center></b>\n",
        "#@markdown <center><img src='https://drfilestreambot.cf/AgADVQ14815' height=\"60\">\n",
        "\n",
        "!pip install bs4\n",
        "\n",
        "import requests\n",
        "import re\n",
        "from urllib.parse import urlparse\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# RECAPTCHA v3 BYPASS\n",
        "# code from https://github.com/xcscxr/Recaptcha-v3-bypass\n",
        "def RecaptchaV3(ANCHOR_URL):\n",
        "    url_base = 'https://www.google.com/recaptcha/'\n",
        "    post_data = \"v={}&reason=q&c={}&k={}&co={}\"\n",
        "    client = requests.Session()\n",
        "    client.headers.update({\n",
        "        'content-type': 'application/x-www-form-urlencoded'\n",
        "    })\n",
        "    matches = re.findall('([api2|enterprise]+)\\/anchor\\?(.*)', ANCHOR_URL)[0]\n",
        "    url_base += matches[0]+'/'\n",
        "    params = matches[1]\n",
        "    res = client.get(url_base+'anchor', params=params)\n",
        "    token = re.findall(r'\"recaptcha-token\" value=\"(.*?)\"', res.text)[0]\n",
        "    params = dict(pair.split('=') for pair in params.split('&'))\n",
        "    post_data = post_data.format(params[\"v\"], token, params[\"k\"], params[\"co\"])\n",
        "    res = client.post(url_base+'reload', params=f'k={params[\"k\"]}', data=post_data)\n",
        "    answer = re.findall(r'\"rresp\",\"(.*?)\"', res.text)[0]    \n",
        "    return answer\n",
        "\n",
        "\n",
        "# code from https://github.com/xcscxr/ouo-bypass/\n",
        "ANCHOR_URL = 'https://www.google.com/recaptcha/api2/anchor?ar=1&k=6Lcr1ncUAAAAAH3cghg6cOTPGARa8adOf-y9zv2x&co=aHR0cHM6Ly9vdW8uaW86NDQz&hl=en&v=1B_yv3CBEV10KtI2HJ6eEXhJ&size=invisible&cb=4xnsug1vufyr'\n",
        "def ouo(url):\n",
        "    client = requests.Session()\n",
        "    tempurl = url.replace(\"ouo.press\", \"ouo.io\")\n",
        "    p = urlparse(tempurl)\n",
        "    id = tempurl.split('/')[-1]\n",
        "    \n",
        "    res = client.get(tempurl)\n",
        "    next_url = f\"{p.scheme}://{p.hostname}/go/{id}\"\n",
        "\n",
        "    for _ in range(2):\n",
        "        if res.headers.get('Location'):\n",
        "            break\n",
        "        bs4 = BeautifulSoup(res.content, 'lxml')\n",
        "        inputs = bs4.form.findAll(\"input\", {\"name\": re.compile(r\"token$\")})\n",
        "        data = { input.get('name'): input.get('value') for input in inputs }\n",
        "        \n",
        "        ans = RecaptchaV3(ANCHOR_URL)\n",
        "        data['x-token'] = ans\n",
        "        h = {\n",
        "            'content-type': 'application/x-www-form-urlencoded'\n",
        "        }\n",
        "        res = client.post(next_url, data=data, headers=h, allow_redirects=False)\n",
        "        next_url = f\"{p.scheme}://{p.hostname}/xreallcygo/{id}\"\n",
        "\n",
        "    return res.headers.get('Location')\n",
        "\n",
        "url = \"https://ouo.press/Zu7Vs5\" #@param {type:\"string\"}\n",
        "print(ouo(url))\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XNKe8fWY0XMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter FileCrypt Link Below!**"
      ],
      "metadata": {
        "id": "Eysjw_9wp8SA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b><center>Enter FileCrypt Link Below</center></b>\n",
        "#@markdown <center><img src='https://drfilestreambot.cf/AgADVw14818' height=\"50\">\n",
        "\n",
        "\n",
        "!pip install bs4\n",
        "!pip install cloudscraper\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import cloudscraper\n",
        "import json\n",
        "\n",
        "client = cloudscraper.create_scraper(allow_brotli=False)\n",
        "\n",
        "\n",
        "# by https://github.com/bipinkrish/filecrypt-bypass\n",
        "def getlinks(dlc):\n",
        "    headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:103.0) Gecko/20100101 Firefox/103.0',\n",
        "    'Accept': 'application/json, text/javascript, */*',\n",
        "    'Accept-Language': 'en-US,en;q=0.5',\n",
        "    # 'Accept-Encoding': 'gzip, deflate',\n",
        "    'X-Requested-With': 'XMLHttpRequest',\n",
        "    'Origin': 'http://dcrypt.it',\n",
        "    'Connection': 'keep-alive',\n",
        "    'Referer': 'http://dcrypt.it/',\n",
        "    }\n",
        "\n",
        "    data = {\n",
        "        'content': dlc,\n",
        "    }\n",
        "\n",
        "    response = client.post('http://dcrypt.it/decrypt/paste', headers=headers, data=data).json()[\"success\"][\"links\"]\n",
        "    links = \"\"\n",
        "    for link in response:\n",
        "        links = links + link + \"\\n\"\n",
        "    print(links)\n",
        "    return links\n",
        "\n",
        "\n",
        "# by https://github.com/bipinkrish/filecrypt-bypass\n",
        "def filecrypt(url):\n",
        "\n",
        "    headers = {\n",
        "    \"authority\": \"filecrypt.co\",\n",
        "    \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
        "    \"accept-language\": \"en-US,en;q=0.9\",\n",
        "    \"cache-control\": \"max-age=0\",\n",
        "    \"content-type\": \"application/x-www-form-urlencoded\",\n",
        "    \"dnt\": \"1\",\n",
        "    \"origin\": \"https://filecrypt.co\",\n",
        "    \"referer\": url,\n",
        "    \"sec-ch-ua\": '\"Google Chrome\";v=\"105\", \"Not)A;Brand\";v=\"8\", \"Chromium\";v=\"105\"',\n",
        "    \"sec-ch-ua-mobile\": \"?0\",\n",
        "    \"sec-ch-ua-platform\": \"Windows\",\n",
        "    \"sec-fetch-dest\": \"document\",\n",
        "    \"sec-fetch-mode\": \"navigate\",\n",
        "    \"sec-fetch-site\": \"same-origin\",\n",
        "    \"sec-fetch-user\": \"?1\",\n",
        "    \"upgrade-insecure-requests\": \"1\",\n",
        "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36\" \n",
        "    }\n",
        "    \n",
        "\n",
        "    resp = client.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(resp.content, \"html.parser\")\n",
        "\n",
        "    buttons = soup.find_all(\"button\")\n",
        "    for ele in buttons:\n",
        "        line = ele.get(\"onclick\")\n",
        "        if line !=None and \"DownloadDLC\" in line:\n",
        "            dlclink = \"https://filecrypt.co/DLC/\" + line.split(\"DownloadDLC('\")[1].split(\"'\")[0] + \".html\"\n",
        "            break\n",
        "\n",
        "    resp = client.get(dlclink,headers=headers)\n",
        "    getlinks(resp.text)\n",
        "\n",
        "\n",
        "url= \"https://filecrypt.co/Container/73F6D9D43B.html\" #@param {type:\"string\"}\n",
        "filecrypt(url)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "s1aVxPqCp63Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter DriveFire Link and it's CRYPT Below!**"
      ],
      "metadata": {
        "id": "WLQhZQdxUpBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b><center>Enter DriveFire Link Below</center></b>\n",
        "#@markdown <center><img src='https://drfilestreambot.cf/AgADe715535' height=\"60\">\n",
        "\n",
        "import requests\n",
        "import re\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "\n",
        "def parse_info(res):\n",
        "    info_parsed = {}\n",
        "    title = re.findall('>(.*?)<\\/h4>', res.text)[0]\n",
        "    info_chunks = re.findall('>(.*?)<\\/td>', res.text)\n",
        "    info_parsed['title'] = title\n",
        "    for i in range(0, len(info_chunks), 2):\n",
        "        info_parsed[info_chunks[i]] = info_chunks[i+1]\n",
        "    return info_parsed\n",
        "\n",
        "def drivefire_dl(url, crypt):\n",
        "    client = requests.Session()\n",
        "    client.cookies.update({'crypt': crypt})\n",
        "    \n",
        "    res = client.get(url)\n",
        "    info_parsed = parse_info(res)\n",
        "    info_parsed['error'] = False\n",
        "    \n",
        "    up = urlparse(url)\n",
        "    req_url = f\"{up.scheme}://{up.netloc}/ajax.php?ajax=download\"\n",
        "    \n",
        "    file_id = url.split('/')[-1]\n",
        "    \n",
        "    data = { 'id': file_id }\n",
        "    \n",
        "    headers = {\n",
        "        'x-requested-with': 'XMLHttpRequest'\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        res = client.post(req_url, headers=headers, data=data).json()['file']\n",
        "    except: return {'error': True, 'src_url': url}\n",
        "    \n",
        "    decoded_id = res.rsplit('/', 1)[-1]\n",
        "    info_parsed = f\"https://drive.google.com/file/d/{decoded_id}\"\n",
        "    return info_parsed\n",
        "    \n",
        "\n",
        "url = \"\" #@param {type:\"string\"}\n",
        "DRIVEFIRE_CRYPT = \"\" #@param {type:\"string\"}\n",
        "print(drivefire_dl(url))\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Xyhc3GBcUoTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter HubDrive Link and it's CRYPT Below!**"
      ],
      "metadata": {
        "id": "JqsWaAOyXec9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b><center>Enter HubDrive Link Below</center></b>\n",
        "#@markdown <center><img src='https://drfilestreambot.cf/AgADgL15537' height=\"60\">\n",
        "\n",
        "import requests\n",
        "import re\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "\n",
        "def parse_info(res):\n",
        "    info_parsed = {}\n",
        "    title = re.findall('>(.*?)<\\/h4>', res.text)[0]\n",
        "    info_chunks = re.findall('>(.*?)<\\/td>', res.text)\n",
        "    info_parsed['title'] = title\n",
        "    for i in range(0, len(info_chunks), 2):\n",
        "        info_parsed[info_chunks[i]] = info_chunks[i+1]\n",
        "    return info_parsed\n",
        "\n",
        "def hubdrive_dl(url):\n",
        "    client = requests.Session()\n",
        "    client.cookies.update({'crypt': crypt})\n",
        "    \n",
        "    res = client.get(url)\n",
        "    info_parsed = parse_info(res)\n",
        "    info_parsed['error'] = False\n",
        "    \n",
        "    up = urlparse(url)\n",
        "    req_url = f\"{up.scheme}://{up.netloc}/ajax.php?ajax=download\"\n",
        "    \n",
        "    file_id = url.split('/')[-1]\n",
        "    \n",
        "    data = { 'id': file_id }\n",
        "    \n",
        "    headers = {\n",
        "        'x-requested-with': 'XMLHttpRequest'\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        res = client.post(req_url, headers=headers, data=data).json()['file']\n",
        "    except: return {'error': True, 'src_url': url}\n",
        "    \n",
        "    gd_id = re.findall('gd=(.*)', res, re.DOTALL)[0]\n",
        "    \n",
        "    info_parsed['gdrive_url'] = f\"https://drive.google.com/open?id={gd_id}\"\n",
        "    info_parsed['src_url'] = url\n",
        "\n",
        "    return info_parsed['gdrive_url']\n",
        "    \n",
        "\n",
        "url = \"\" #@param {type:\"string\"}\n",
        "HUBDRIVE_CRYPT = \"\" #@param {type:\"string\"}\n",
        "print(hubdrive_dl(url))\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "yZngJVqcXdvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter KatDrive Link and it's CRYPT Below!**"
      ],
      "metadata": {
        "id": "YiwuJbjrZEYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b><center>Enter KatDrive Link Below</center></b>\n",
        "#@markdown <center><img src='https://drfilestreambot.cf/AgADgr15540' height=\"60\">\n",
        "\n",
        "import requests\n",
        "import re\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "\n",
        "def parse_info(res):\n",
        "    info_parsed = {}\n",
        "    title = re.findall('>(.*?)<\\/h4>', res.text)[0]\n",
        "    info_chunks = re.findall('>(.*?)<\\/td>', res.text)\n",
        "    info_parsed['title'] = title\n",
        "    for i in range(0, len(info_chunks), 2):\n",
        "        info_parsed[info_chunks[i]] = info_chunks[i+1]\n",
        "    return info_parsed\n",
        "\n",
        "def katdrive_dl(url):\n",
        "    client = requests.Session()\n",
        "    client.cookies.update({'crypt': crypt})\n",
        "    \n",
        "    res = client.get(url)\n",
        "    info_parsed = parse_info(res)\n",
        "    info_parsed['error'] = False\n",
        "    \n",
        "    up = urlparse(url)\n",
        "    req_url = f\"{up.scheme}://{up.netloc}/ajax.php?ajax=download\"\n",
        "    \n",
        "    file_id = url.split('/')[-1]\n",
        "    \n",
        "    data = { 'id': file_id }\n",
        "    \n",
        "    headers = {\n",
        "        'x-requested-with': 'XMLHttpRequest'\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        res = client.post(req_url, headers=headers, data=data).json()['file']\n",
        "    except: return {'error': True, 'src_url': url}\n",
        "    \n",
        "    gd_id = re.findall('gd=(.*)', res, re.DOTALL)[0]\n",
        "    \n",
        "    info_parsed['gdrive_url'] = f\"https://drive.google.com/open?id={gd_id}\"\n",
        "    info_parsed['src_url'] = url\n",
        "\n",
        "    return info_parsed['gdrive_url']\n",
        "    \n",
        "\n",
        "url = \"\" #@param {type:\"string\"}\n",
        "KATDRIVE_CRYPT = \"\" #@param {type:\"string\"}\n",
        "print(katdrive_dl(url))\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "VK3C4ip4ZDuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter Kolop Link and it's CRYPT Below!**"
      ],
      "metadata": {
        "id": "oMCnm8lfaSyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b><center>Enter Kolop Link Below</center></b>\n",
        "#@markdown <center><img src='https://drfilestreambot.cf/AgADhL15546' height=\"60\">\n",
        "\n",
        "import requests\n",
        "import re\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "\n",
        "def parse_info(res):\n",
        "    info_parsed = {}\n",
        "    title = re.findall('>(.*?)<\\/h4>', res.text)[0]\n",
        "    info_chunks = re.findall('>(.*?)<\\/td>', res.text)\n",
        "    info_parsed['title'] = title\n",
        "    for i in range(0, len(info_chunks), 2):\n",
        "        info_parsed[info_chunks[i]] = info_chunks[i+1]\n",
        "    return info_parsed\n",
        "\n",
        "def kolop_dl(url):\n",
        "    client = requests.Session()\n",
        "    client.cookies.update({'crypt': crypt})\n",
        "    \n",
        "    res = client.get(url)\n",
        "    info_parsed = parse_info(res)\n",
        "    info_parsed['error'] = False\n",
        "    \n",
        "    up = urlparse(url)\n",
        "    req_url = f\"{up.scheme}://{up.netloc}/ajax.php?ajax=download\"\n",
        "    \n",
        "    file_id = url.split('/')[-1]\n",
        "    \n",
        "    data = { 'id': file_id }\n",
        "    \n",
        "    headers = {\n",
        "        'x-requested-with': 'XMLHttpRequest'\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        res = client.post(req_url, headers=headers, data=data).json()['file']\n",
        "    except: return {'error': True, 'src_url': url}\n",
        "    \n",
        "    gd_id = re.findall('gd=(.*)', res, re.DOTALL)[0]\n",
        "    \n",
        "    info_parsed['gdrive_url'] = f\"https://drive.google.com/open?id={gd_id}\"\n",
        "    info_parsed['src_url'] = url\n",
        "\n",
        "    return info_parsed['gdrive_url']\n",
        "    \n",
        "\n",
        "url = \"\" #@param {type:\"string\"}\n",
        "KOLOP_CRYPT = \"\" #@param {type:\"string\"}\n",
        "print(kolop_dl(url))\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Rbdt0oomaR2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DDL Generator**\n",
        "\n",
        "disk.yandex.com,\n",
        "mediafire.com,\n",
        "uptobox.com,\n",
        "osdn.net,\n",
        "github.com,\n",
        "hxfile.co,\n",
        "anonfiles.com,\n",
        "letsupload.io,\n",
        "1drv.ms(onedrive),\n",
        "pixeldrain.com,\n",
        "antfiles.com,\n",
        "streamtape.com,\n",
        "bayfiles.com,\n",
        "racaty.net,\n",
        "1fichier.com,\n",
        "solidfiles.com,\n",
        "krakenfiles.com,\n",
        "upload.ee,\n",
        "mdisk.me,\n",
        "wetransfer.com,\n",
        "gofile.io,\n",
        "dropbox.com,\n",
        "zippyshare.com,\n",
        "megaup.net,\n",
        "fembed.net, fembed.com, femax20.com, fcdn.stream, feurl.com, layarkacaxxi.icu, naniplay.nanime.in, naniplay.nanime.biz, naniplay.com, mm9842.com,\n",
        "sbembed.com, watchsb.com, streamsb.net, sbplay.org."
      ],
      "metadata": {
        "id": "Npkq2uQ8NRfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this is for upgrading python version\n",
        "\n",
        "!wget -O mini.sh https://repo.anaconda.com/miniconda/Miniconda3-py38_4.8.2-Linux-x86_64.sh\n",
        "!chmod +x mini.sh\n",
        "!bash ./mini.sh -b -f -p /usr/local\n",
        "!conda install -q -y jupyter\n",
        "!conda install -q -y google-colab -c conda-forge\n",
        "!python -m ipykernel install --name \"py38\" --user"
      ],
      "metadata": {
        "id": "mBO3cLx6c6VE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b><center>Enter Link Below (Reload the web page and execute this cell) </center></b>\n",
        "#@markdown <center><img src='https://drfilestreambot.cf/AgADD716907' height=\"70\">\n",
        "\n",
        "!pip install requests lk21 cfscrape bs4 cloudscraper hashlib\n",
        "\n",
        "from requests import get as rget, head as rhead, post as rpost, Session as rsession\n",
        "from re import findall as re_findall, sub as re_sub, match as re_match, search as re_search\n",
        "from urllib.parse import urlparse, unquote\n",
        "from json import loads as jsonloads\n",
        "from lk21 import Bypass\n",
        "from cfscrape import create_scraper\n",
        "from bs4 import BeautifulSoup\n",
        "from base64 import standard_b64encode\n",
        "from time import sleep\n",
        "import cloudscraper\n",
        "import hashlib\n",
        "import requests\n",
        "\n",
        "class DirectDownloadLinkException(Exception):\n",
        "    \"\"\"Not method found for extracting direct download link from the http link\"\"\"\n",
        "    pass\n",
        "\n",
        "def yandex_disk(url: str) -> str:\n",
        "    \"\"\" Yandex.Disk direct link generator\n",
        "    Based on https://github.com/wldhx/yadisk-direct \"\"\"\n",
        "    try:\n",
        "        link = re_findall(r'\\b(https?://(yadi.sk|disk.yandex.com)\\S+)', url)[0][0]\n",
        "    except IndexError:\n",
        "        return \"No Yandex.Disk links found\\n\"\n",
        "    api = 'https://cloud-api.yandex.net/v1/disk/public/resources/download?public_key={}'\n",
        "    try:\n",
        "        return rget(api.format(link)).json()['href']\n",
        "    except KeyError:\n",
        "        raise DirectDownloadLinkException(\"ERROR: File not found/Download limit reached\")\n",
        "\n",
        "def uptobox(url: str, UPTOBOX_TOKEN: str) -> str:\n",
        "    \"\"\" Uptobox direct link generator\n",
        "    based on https://github.com/jovanzers/WinTenCermin and https://github.com/sinoobie/noobie-mirror \"\"\"\n",
        "    try:\n",
        "        link = re_findall(r'\\bhttps?://.*uptobox\\.com\\S+', url)[0]\n",
        "    except IndexError:\n",
        "        raise DirectDownloadLinkException(\"No Uptobox links found\")\n",
        "    if UPTOBOX_TOKEN is None:\n",
        "        print('UPTOBOX_TOKEN not provided!')\n",
        "        dl_url = link\n",
        "    else:\n",
        "        try:\n",
        "            link = re_findall(r'\\bhttp?://.*uptobox\\.com/dl\\S+', url)[0]\n",
        "            dl_url = link\n",
        "        except:\n",
        "            file_id = re_findall(r'\\bhttps?://.*uptobox\\.com/(\\w+)', url)[0]\n",
        "            file_link = f'https://uptobox.com/api/link?token={UPTOBOX_TOKEN}&file_code={file_id}'\n",
        "            req = rget(file_link)\n",
        "            result = req.json()\n",
        "            if result['message'].lower() == 'success':\n",
        "                dl_url = result['data']['dlLink']\n",
        "            elif result['message'].lower() == 'waiting needed':\n",
        "                waiting_time = result[\"data\"][\"waiting\"] + 1\n",
        "                waiting_token = result[\"data\"][\"waitingToken\"]\n",
        "                sleep(waiting_time)\n",
        "                req2 = rget(f\"{file_link}&waitingToken={waiting_token}\")\n",
        "                result2 = req2.json()\n",
        "                dl_url = result2['data']['dlLink']\n",
        "            elif result['message'].lower() == 'you need to wait before requesting a new download link':\n",
        "                cooldown = divmod(result['data']['waiting'], 60)\n",
        "                raise DirectDownloadLinkException(f\"ERROR: Uptobox is being limited please wait {cooldown[0]} min {cooldown[1]} sec.\")\n",
        "            else:\n",
        "                print(f\"UPTOBOX_ERROR: {result}\")\n",
        "                raise DirectDownloadLinkException(f\"ERROR: {result['message']}\")\n",
        "    return dl_url\n",
        "\n",
        "def mediafire(url: str) -> str:\n",
        "    \"\"\" MediaFire direct link generator \"\"\"\n",
        "    try:\n",
        "        link = re_findall(r'\\bhttps?://.*mediafire\\.com\\S+', url)[0]\n",
        "    except IndexError:\n",
        "        raise DirectDownloadLinkException(\"No MediaFire links found\")\n",
        "    page = BeautifulSoup(rget(link).content, 'html.parser')\n",
        "    info = page.find('a', {'aria-label': 'Download file'})\n",
        "    return info.get('href')\n",
        "\n",
        "def osdn(url: str) -> str:\n",
        "    \"\"\" OSDN direct link generator \"\"\"\n",
        "    osdn_link = 'https://osdn.net'\n",
        "    try:\n",
        "        link = re_findall(r'\\bhttps?://.*osdn\\.net\\S+', url)[0]\n",
        "    except IndexError:\n",
        "        raise DirectDownloadLinkException(\"No OSDN links found\")\n",
        "    page = BeautifulSoup(\n",
        "        rget(link, allow_redirects=True).content, 'html.parser')\n",
        "    info = page.find('a', {'class': 'mirror_link'})\n",
        "    link = unquote(osdn_link + info['href'])\n",
        "    mirrors = page.find('form', {'id': 'mirror-select-form'}).findAll('tr')\n",
        "    urls = []\n",
        "    for data in mirrors[1:]:\n",
        "        mirror = data.find('input')['value']\n",
        "        urls.append(re_sub(r'm=(.*)&f', f'm={mirror}&f', link))\n",
        "    return urls[0]\n",
        "\n",
        "def github(url: str) -> str:\n",
        "    \"\"\" GitHub direct links generator \"\"\"\n",
        "    try:\n",
        "        re_findall(r'\\bhttps?://.*github\\.com.*releases\\S+', url)[0]\n",
        "    except IndexError:\n",
        "        raise DirectDownloadLinkException(\"No GitHub Releases links found\")\n",
        "    download = rget(url, stream=True, allow_redirects=False)\n",
        "    try:\n",
        "        return download.headers[\"location\"]\n",
        "    except KeyError:\n",
        "        raise DirectDownloadLinkException(\"ERROR: Can't extract the link\")\n",
        "\n",
        "def hxfile(url: str) -> str:\n",
        "    \"\"\" Hxfile direct link generator\n",
        "    Based on https://github.com/zevtyardt/lk21\n",
        "    \"\"\"\n",
        "    return Bypass().bypass_filesIm(url)\n",
        "\n",
        "def anonfiles(url: str) -> str:\n",
        "    \"\"\" Anonfiles direct link generator\n",
        "    Based on https://github.com/zevtyardt/lk21\n",
        "    \"\"\"\n",
        "    return Bypass().bypass_anonfiles(url)\n",
        "\n",
        "def letsupload(url: str) -> str:\n",
        "    \"\"\" Letsupload direct link generator\n",
        "    Based on https://github.com/zevtyardt/lk21\n",
        "    \"\"\"\n",
        "    try:\n",
        "        link = re_findall(r'\\bhttps?://.*letsupload\\.io\\S+', url)[0]\n",
        "    except IndexError:\n",
        "        raise DirectDownloadLinkException(\"No Letsupload links found\\n\")\n",
        "    return Bypass().bypass_url(link)\n",
        "\n",
        "def fembed(link: str) -> str:\n",
        "    \"\"\" Fembed direct link generator\n",
        "    Based on https://github.com/zevtyardt/lk21\n",
        "    \"\"\"\n",
        "    dl_url= Bypass().bypass_fembed(link)\n",
        "    count = len(dl_url)\n",
        "    lst_link = [dl_url[i] for i in dl_url]\n",
        "    return lst_link[count-1]\n",
        "\n",
        "def sbembed(link: str) -> str:\n",
        "    \"\"\" Sbembed direct link generator\n",
        "    Based on https://github.com/zevtyardt/lk21\n",
        "    \"\"\"\n",
        "    dl_url= Bypass().bypass_sbembed(link)\n",
        "    count = len(dl_url)\n",
        "    lst_link = [dl_url[i] for i in dl_url]\n",
        "    return lst_link[count-1]\n",
        "\n",
        "def onedrive(link: str) -> str:\n",
        "    \"\"\" Onedrive direct link generator\n",
        "    Based on https://github.com/UsergeTeam/Userge \"\"\"\n",
        "    link_without_query = urlparse(link)._replace(query=None).geturl()\n",
        "    direct_link_encoded = str(standard_b64encode(bytes(link_without_query, \"utf-8\")), \"utf-8\")\n",
        "    direct_link1 = f\"https://api.onedrive.com/v1.0/shares/u!{direct_link_encoded}/root/content\"\n",
        "    resp = rhead(direct_link1)\n",
        "    if resp.status_code != 302:\n",
        "        raise DirectDownloadLinkException(\"ERROR: Unauthorized link, the link may be private\")\n",
        "    return resp.next.url\n",
        "\n",
        "def pixeldrain(url: str) -> str:\n",
        "    \"\"\" Based on https://github.com/yash-dk/TorToolkit-Telegram \"\"\"\n",
        "    url = url.strip(\"/ \")\n",
        "    file_id = url.split(\"/\")[-1]\n",
        "    if url.split(\"/\")[-2] == \"l\":\n",
        "        info_link = f\"https://pixeldrain.com/api/list/{file_id}\"\n",
        "        dl_link = f\"https://pixeldrain.com/api/list/{file_id}/zip\"\n",
        "    else:\n",
        "        info_link = f\"https://pixeldrain.com/api/file/{file_id}/info\"\n",
        "        dl_link = f\"https://pixeldrain.com/api/file/{file_id}\"\n",
        "    resp = rget(info_link).json()\n",
        "    if resp[\"success\"]:\n",
        "        return dl_link\n",
        "    else:\n",
        "        raise DirectDownloadLinkException(f\"ERROR: Cant't download due {resp['message']}.\")\n",
        "\n",
        "def antfiles(url: str) -> str:\n",
        "    \"\"\" Antfiles direct link generator\n",
        "    Based on https://github.com/zevtyardt/lk21\n",
        "    \"\"\"\n",
        "    return Bypass().bypass_antfiles(url)\n",
        "\n",
        "def streamtape(url: str) -> str:\n",
        "    \"\"\" Streamtape direct link generator\n",
        "    Based on https://github.com/zevtyardt/lk21\n",
        "    \"\"\"\n",
        "    return Bypass().bypass_streamtape(url)\n",
        "\n",
        "def racaty(url: str) -> str:\n",
        "    \"\"\" Racaty direct link generator\n",
        "    based on https://github.com/SlamDevs/slam-mirrorbot\"\"\"\n",
        "    dl_url = ''\n",
        "    try:\n",
        "        re_findall(r'\\bhttps?://.*racaty\\.net\\S+', url)[0]\n",
        "    except IndexError:\n",
        "        raise DirectDownloadLinkException(\"No Racaty links found\")\n",
        "    scraper = create_scraper()\n",
        "    r = scraper.get(url)\n",
        "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "    op = soup.find(\"input\", {\"name\": \"op\"})[\"value\"]\n",
        "    ids = soup.find(\"input\", {\"name\": \"id\"})[\"value\"]\n",
        "    rapost = scraper.post(url, data = {\"op\": op, \"id\": ids})\n",
        "    rsoup = BeautifulSoup(rapost.text, \"html.parser\")\n",
        "    dl_url = rsoup.find(\"a\", {\"id\": \"uniqueExpirylink\"})[\"href\"].replace(\" \", \"%20\")\n",
        "    return dl_url\n",
        "\n",
        "def fichier(link: str) -> str:\n",
        "    \"\"\" 1Fichier direct link generator\n",
        "    Based on https://github.com/Maujar\n",
        "    \"\"\"\n",
        "    regex = r\"^([http:\\/\\/|https:\\/\\/]+)?.*1fichier\\.com\\/\\?.+\"\n",
        "    gan = re_match(regex, link)\n",
        "    if not gan:\n",
        "      raise DirectDownloadLinkException(\"ERROR: The link you entered is wrong!\")\n",
        "    if \"::\" in link:\n",
        "      pswd = link.split(\"::\")[-1]\n",
        "      url = link.split(\"::\")[-2]\n",
        "    else:\n",
        "      pswd = None\n",
        "      url = link\n",
        "    try:\n",
        "      if pswd is None:\n",
        "        req = rpost(url)\n",
        "      else:\n",
        "        pw = {\"pass\": pswd}\n",
        "        req = rpost(url, data=pw)\n",
        "    except:\n",
        "      raise DirectDownloadLinkException(\"ERROR: Unable to reach 1fichier server!\")\n",
        "    if req.status_code == 404:\n",
        "      raise DirectDownloadLinkException(\"ERROR: File not found/The link you entered is wrong!\")\n",
        "    soup = BeautifulSoup(req.content, 'html.parser')\n",
        "    if soup.find(\"a\", {\"class\": \"ok btn-general btn-orange\"}) is not None:\n",
        "        dl_url = soup.find(\"a\", {\"class\": \"ok btn-general btn-orange\"})[\"href\"]\n",
        "        if dl_url is None:\n",
        "          raise DirectDownloadLinkException(\"ERROR: Unable to generate Direct Link 1fichier!\")\n",
        "        else:\n",
        "          return dl_url\n",
        "    elif len(soup.find_all(\"div\", {\"class\": \"ct_warn\"})) == 3:\n",
        "        str_2 = soup.find_all(\"div\", {\"class\": \"ct_warn\"})[-1]\n",
        "        if \"you must wait\" in str(str_2).lower():\n",
        "            numbers = [int(word) for word in str(str_2).split() if word.isdigit()]\n",
        "            if not numbers:\n",
        "                raise DirectDownloadLinkException(\"ERROR: 1fichier is on a limit. Please wait a few minutes/hour.\")\n",
        "            else:\n",
        "                raise DirectDownloadLinkException(f\"ERROR: 1fichier is on a limit. Please wait {numbers[0]} minute.\")\n",
        "        elif \"protect access\" in str(str_2).lower():\n",
        "          raise DirectDownloadLinkException(f\"ERROR: This link requires a password!\\n\\n<b>This link requires a password!</b>\\n- Insert sign <b>::</b> after the link and write the password after the sign.\\n\\n<b>Example:</b> https://1fichier.com/?smmtd8twfpm66awbqz04::love you\\n\\n* No spaces between the signs <b>::</b>\\n* For the password, you can use a space!\")\n",
        "        else:\n",
        "            print(str_2)\n",
        "            raise DirectDownloadLinkException(\"ERROR: Failed to generate Direct Link from 1fichier!\")\n",
        "    elif len(soup.find_all(\"div\", {\"class\": \"ct_warn\"})) == 4:\n",
        "        str_1 = soup.find_all(\"div\", {\"class\": \"ct_warn\"})[-2]\n",
        "        str_3 = soup.find_all(\"div\", {\"class\": \"ct_warn\"})[-1]\n",
        "        if \"you must wait\" in str(str_1).lower():\n",
        "            numbers = [int(word) for word in str(str_1).split() if word.isdigit()]\n",
        "            if not numbers:\n",
        "                raise DirectDownloadLinkException(\"ERROR: 1fichier is on a limit. Please wait a few minutes/hour.\")\n",
        "            else:\n",
        "                raise DirectDownloadLinkException(f\"ERROR: 1fichier is on a limit. Please wait {numbers[0]} minute.\")\n",
        "        elif \"bad password\" in str(str_3).lower():\n",
        "          raise DirectDownloadLinkException(\"ERROR: The password you entered is wrong!\")\n",
        "        else:\n",
        "            raise DirectDownloadLinkException(\"ERROR: Error trying to generate Direct Link from 1fichier!\")\n",
        "    else:\n",
        "        raise DirectDownloadLinkException(\"ERROR: Error trying to generate Direct Link from 1fichier!\")\n",
        "\n",
        "def solidfiles(url: str) -> str:\n",
        "    \"\"\" Solidfiles direct link generator\n",
        "    Based on https://github.com/Xonshiz/SolidFiles-Downloader\n",
        "    By https://github.com/Jusidama18 \"\"\"\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36'\n",
        "    }\n",
        "    pageSource = rget(url, headers = headers).text\n",
        "    mainOptions = str(re_search(r'viewerOptions\\'\\,\\ (.*?)\\)\\;', pageSource).group(1))\n",
        "    return jsonloads(mainOptions)[\"downloadUrl\"]\n",
        "\n",
        "def krakenfiles(page_link: str) -> str:\n",
        "    \"\"\" krakenfiles direct link generator\n",
        "    Based on https://github.com/tha23rd/py-kraken\n",
        "    By https://github.com/junedkh \"\"\"\n",
        "    page_resp = rsession().get(page_link)\n",
        "    soup = BeautifulSoup(page_resp.text, \"html.parser\")\n",
        "    try:\n",
        "        token = soup.find(\"input\", id=\"dl-token\")[\"value\"]\n",
        "    except:\n",
        "        raise DirectDownloadLinkException(f\"Page link is wrong: {page_link}\")\n",
        "\n",
        "    hashes = [\n",
        "        item[\"data-file-hash\"]\n",
        "        for item in soup.find_all(\"div\", attrs={\"data-file-hash\": True})\n",
        "    ]\n",
        "    if not hashes:\n",
        "        raise DirectDownloadLinkException(f\"ERROR: Hash not found for : {page_link}\")\n",
        "\n",
        "    dl_hash = hashes[0]\n",
        "\n",
        "    payload = f'------WebKitFormBoundary7MA4YWxkTrZu0gW\\r\\nContent-Disposition: form-data; name=\"token\"\\r\\n\\r\\n{token}\\r\\n------WebKitFormBoundary7MA4YWxkTrZu0gW--'\n",
        "    headers = {\n",
        "        \"content-type\": \"multipart/form-data; boundary=----WebKitFormBoundary7MA4YWxkTrZu0gW\",\n",
        "        \"cache-control\": \"no-cache\",\n",
        "        \"hash\": dl_hash,\n",
        "    }\n",
        "\n",
        "    dl_link_resp = rsession().post(\n",
        "        f\"https://krakenfiles.com/download/{hash}\", data=payload, headers=headers)\n",
        "\n",
        "    dl_link_json = dl_link_resp.json()\n",
        "\n",
        "    if \"url\" in dl_link_json:\n",
        "        return dl_link_json[\"url\"]\n",
        "    else:\n",
        "        raise DirectDownloadLinkException(f\"ERROR: Failed to acquire download URL from kraken for : {page_link}\")\n",
        "\n",
        "def uploadee(url: str) -> str:\n",
        "    \"\"\" uploadee direct link generator\n",
        "    By https://github.com/iron-heart-x\"\"\"\n",
        "    try:\n",
        "        soup = BeautifulSoup(rget(url).content, 'html.parser')\n",
        "        sa = soup.find('a', attrs={'id':'d_l'})\n",
        "        return sa['href']\n",
        "    except:\n",
        "        raise DirectDownloadLinkException(f\"ERROR: Failed to acquire download URL from upload.ee for : {url}\")\n",
        "\n",
        "def mdisk(url):\n",
        "    api = \"https://api.emilyx.in/api\"\n",
        "    client = cloudscraper.create_scraper(allow_brotli=False)\n",
        "    resp = client.get(url)\n",
        "    if resp.status_code == 404:\n",
        "        return \"File not found/The link you entered is wrong!\"\n",
        "    try:\n",
        "        resp = client.post(api, json={\"type\": \"mdisk\", \"url\": url})\n",
        "        res = resp.json()\n",
        "    except BaseException:\n",
        "        return \"API UnResponsive / Invalid Link !\"\n",
        "    if res[\"success\"] is True:\n",
        "        return res[\"url\"]\n",
        "    else:\n",
        "        return res[\"msg\"]\n",
        "\n",
        "def wetransfer(url):\n",
        "    api = \"https://api.emilyx.in/api\"\n",
        "    client = cloudscraper.create_scraper(allow_brotli=False)\n",
        "    resp = client.get(url)\n",
        "    if resp.status_code == 404:\n",
        "        return \"File not found/The link you entered is wrong!\"\n",
        "    try:\n",
        "        resp = client.post(api, json={\"type\": \"wetransfer\", \"url\": url})\n",
        "        res = resp.json()\n",
        "    except BaseException:\n",
        "        return \"API UnResponsive / Invalid Link !\"\n",
        "    if res[\"success\"] is True:\n",
        "        return res[\"url\"]\n",
        "    else:\n",
        "        return res[\"msg\"]\n",
        "\n",
        "def gofile_dl(url,password=\"\"):\n",
        "    api_uri = 'https://api.gofile.io'\n",
        "    client = requests.Session()\n",
        "    res = client.get(api_uri+'/createAccount').json()\n",
        "    \n",
        "    data = {\n",
        "        'contentId': url.split('/')[-1],\n",
        "        'token': res['data']['token'],\n",
        "        'websiteToken': '12345',\n",
        "        'cache': 'true',\n",
        "        'password': hashlib.sha256(password.encode('utf-8')).hexdigest()\n",
        "    }\n",
        "    res = client.get(api_uri+'/getContent', params=data).json()\n",
        "\n",
        "    content = []\n",
        "    for item in res['data']['contents'].values():\n",
        "        content.append(item)\n",
        "    \n",
        "    return {\n",
        "        'accountToken': data['token'],\n",
        "        'files': content\n",
        "    }[\"files\"][0][\"link\"]\n",
        "\n",
        "def dropbox(url):\n",
        "    return url.replace(\"www.\",\"\").replace(\"dropbox.com\",\"dl.dropboxusercontent.com\").replace(\"?dl=0\",\"\")\n",
        "\n",
        "def zippyshare(url):\n",
        "    resp = requests.get(url).text\n",
        "    surl = resp.split(\"document.getElementById('dlbutton').href = \")[1].split(\";\")[0]\n",
        "    parts = surl.split(\"(\")[1].split(\")\")[0].split(\" \")\n",
        "    val = str(int(parts[0]) % int(parts[2]) + int(parts[4]) % int(parts[6]))\n",
        "    surl = surl.split('\"')\n",
        "    burl = url.split(\"zippyshare.com\")[0]\n",
        "    furl = burl + \"zippyshare.com\" + surl[1] + val + surl[-2]\n",
        "    print(furl)\n",
        "\n",
        "def megaup(url):\n",
        "    api = \"https://api.emilyx.in/api\"\n",
        "    client = cloudscraper.create_scraper(allow_brotli=False)\n",
        "    resp = client.get(url)\n",
        "    if resp.status_code == 404:\n",
        "        return \"File not found/The link you entered is wrong!\"\n",
        "    try:\n",
        "        resp = client.post(api, json={\"type\": \"megaup\", \"url\": url})\n",
        "        res = resp.json()\n",
        "    except BaseException:\n",
        "        return \"API UnResponsive / Invalid Link !\"\n",
        "    if res[\"success\"] is True:\n",
        "        return res[\"url\"]\n",
        "    else:\n",
        "        return res[\"msg\"]\n",
        "\n",
        "def direct_link_generator(link: str):\n",
        "    \"\"\" direct links generator \"\"\"\n",
        "    if 'yadi.sk' in link or 'disk.yandex.com' in link:\n",
        "        return yandex_disk(link)\n",
        "    elif 'mediafire.com' in link:\n",
        "        return mediafire(link)\n",
        "    elif 'uptobox.com' in link:\n",
        "        return uptobox(link,UPTOBOX_TOKEN)\n",
        "    elif 'osdn.net' in link:\n",
        "        return osdn(link)\n",
        "    elif 'github.com' in link:\n",
        "        return github(link)\n",
        "    elif 'hxfile.co' in link:\n",
        "        return hxfile(link)\n",
        "    elif 'anonfiles.com' in link:\n",
        "        return anonfiles(link)\n",
        "    elif 'letsupload.io' in link:\n",
        "        return letsupload(link)\n",
        "    elif '1drv.ms' in link:\n",
        "        return onedrive(link)\n",
        "    elif 'pixeldrain.com' in link:\n",
        "        return pixeldrain(link)\n",
        "    elif 'antfiles.com' in link:\n",
        "        return antfiles(link)\n",
        "    elif 'streamtape.com' in link:\n",
        "        return streamtape(link)\n",
        "    elif 'bayfiles.com' in link:\n",
        "        return anonfiles(link)\n",
        "    elif 'racaty.net' in link:\n",
        "        return racaty(link)\n",
        "    elif '1fichier.com' in link:\n",
        "        return fichier(link)\n",
        "    elif 'solidfiles.com' in link:\n",
        "        return solidfiles(link)\n",
        "    elif 'krakenfiles.com' in link:\n",
        "        return krakenfiles(link)\n",
        "    elif 'upload.ee' in link:\n",
        "        return uploadee(link)\n",
        "    elif 'mdisk.me' in link:\n",
        "        return mdisk(link)\n",
        "    elif 'wetransfer.com' in link:\n",
        "        return wetransfer(link)\n",
        "    elif 'gofile.io' in link:\n",
        "        return gofile_dl(link,GO_FILE_PASS)\n",
        "    elif 'dropbox.com' in link:\n",
        "        return dropbox(link)\n",
        "    elif 'zippyshare.com' in link:\n",
        "        return zippyshare(link)\n",
        "    elif 'megaup.net' in link:\n",
        "        return megaup(link)\n",
        "    elif any(x in link for x in fmed_list):\n",
        "        return fembed(link)\n",
        "    elif any(x in link for x in ['sbembed.com', 'watchsb.com', 'streamsb.net', 'sbplay.org']):\n",
        "        return sbembed(link)\n",
        "    else:\n",
        "        raise DirectDownloadLinkException(f'No Direct link function found for {link}')\n",
        "\n",
        "\n",
        "supported_sites_list = \"disk.yandex.com\\nmediafire.com\\nuptobox.com\\nosdn.net\\ngithub.com\\nhxfile.co\\nanonfiles.com\\nletsupload.io\\n1drv.ms(onedrive)\\n\\\n",
        "pixeldrain.com\\nantfiles.com\\nstreamtape.com\\nbayfiles.com\\nracaty.net\\n1fichier.com\\nsolidfiles.com\\nkrakenfiles.com\\n\\\n",
        "upload.ee\\nmdisk.me\\nwetransfer.com\\ngofile.io\\ndropbox.com\\nzippyshare.com\\nmegaup.net\\n\\\n",
        "fembed.net, fembed.com, femax20.com, fcdn.stream, feurl.com, layarkacaxxi.icu, naniplay.nanime.in, naniplay.nanime.biz, naniplay.com, mm9842.com\\n\\\n",
        "sbembed.com, watchsb.com, streamsb.net, sbplay.org\"\n",
        "\n",
        "fmed_list = ['fembed.net', 'fembed.com', 'femax20.com', 'fcdn.stream', 'feurl.com', 'layarkacaxxi.icu',\n",
        "             'naniplay.nanime.in', 'naniplay.nanime.biz', 'naniplay.com', 'mm9842.com']\n",
        "\n",
        "url = \"https://1fichier.com/?95st2q2p8gbix9r4rlib\" #@param {type:\"string\"}\n",
        "UPTOBOX_TOKEN = \"OPTIONAL\" #@param {type:\"string\"}\n",
        "GO_FILE_PASS = \"OPTIONAL\" #@param {type:\"string\"}\n",
        "\n",
        "print(direct_link_generator(url))"
      ],
      "metadata": {
        "id": "lLQF6jtkNLl8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
